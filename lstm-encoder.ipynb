{
  "cells": [
    {
      "metadata": {
        "_uuid": "25cfaf08fb71c7defc98d4044bcce83e72929c4a"
      },
      "cell_type": "markdown",
      "source": "To generate subword vectors, we use Google's sentencepiece (https://github.com/google/sentencepiece) with pretrained byte-pair encoding language models trained on wikipedia (https://github.com/bheinzerling/bpemb).\n\nThe cleaned sentences are lowercase only, unified all digits to 0, rid of URL, and with some additional manual anomaly removal.\n\nTo generate the subword sequences, we run this following line in Terminal (e.g. merge op = 3000)\nspm_encode --model en.wiki.bpe.op3000.model < amazon_sentences.txt.clean > amazon_sentences.bpe3000"
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Sentence Embedding with LSTM\n# trained/tested with Amazon review dataset\n\n# Load the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Masking\nfrom keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\nfrom keras.callbacks import ModelCheckpoint\n\n# The dimensionality of all sentence vectors\nvector_dim = 25",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "94982abc41f374ac32725413b6d7e4a3b9016f53",
        "_cell_guid": "f654568d-d9c6-4ed7-a088-e51af13ca16e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from gensim.models import KeyedVectors\nimport codecs\n\n# Load the pre-trained subwrod language model and subword sequences\n# Training data and test data are preemptively concatenated in one single training file for convenience\n# First 40k for training, last 10k for testing\n\nmodel = KeyedVectors.load_word2vec_format(\"../input/en.wiki.bpe.op3000.d\" + str(vector_dim) + \".w2v.bin\", binary=True)\n# f_train = codecs.open(\"../input/movie_phrases.bpe3000\", \"r\", \"utf-8\")\nf_train = codecs.open(\"../input/amazon_train_sentences.bpe3000\", \"r\", \"utf-8\")\namazon_train = [line for line in f_train.readlines() if line.strip()]\nf_train.close()\n\n# Same goes for loading the labels\nsentiment_train = np.genfromtxt('../input/amazon_train_labels.txt', delimiter=',')\n# sentiment_train = np.genfromtxt('../input/movie_labels.txt', delimiter=',')\nnum_labels = len(np.unique(sentiment_train))\n# The amazon dataset has 2 labels, while the Kaggle Movie review dataset has 5\n\nbatch_size = 256  # Batch size for training\nepochs = 10  # Number of epochs to train for\nnum_samples = 50000 # Total number of samples used\nnum_split = 40000\n\namazon_train = amazon_train[:num_samples] \nsentiment_train = sentiment_train[:num_samples]\namazon_train_labels = np_utils.to_categorical(sentiment_train, num_labels)\n\n# Due to memory limitation, we only take in the first 800 subwords of each sentence\n# for zero padding\nmax_encoder_seq_length = 800\n\ngc.collect()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "eb0130b1ec5148427253c77eccf618b05d21f3e3",
        "_cell_guid": "645aec11-9cfe-4229-a23d-f68f47a68a74",
        "trusted": true
      },
      "cell_type": "code",
      "source": "encoder_input_data = np.zeros(\n    (num_samples, max_encoder_seq_length, vector_dim),\n    dtype='float32')\n\n# Feed in the subword vectors\nfor i in range(len(amazon_train)):\n    try:\n        input_vec = model[amazon_train[i].split()]\n    except KeyError:\n        # In case of non-existing vocabulary, we simply replace it with the previous subword\n        # Very rare\n        print(i)\n        input_vec = model[amazon_train[i-1].split()]\n    \n    for j in range(len(input_vec)):\n        # The zero-padding for LSTM is to align the sequences \"to the right\"\n        # so that the hidden state in the last iteration is close to the\n        # last word fed into the LSTM\n        encoder_input_data[i][j - len(input_vec)] = + input_vec[j] \n        \nprint(encoder_input_data[0])",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[ 0.        0.        0.       ...  0.        0.        0.      ]\n [ 0.        0.        0.       ...  0.        0.        0.      ]\n [ 0.        0.        0.       ...  0.        0.        0.      ]\n ...\n [ 0.156406  0.085686 -0.056812 ...  0.400155 -0.178166 -0.299723]\n [ 0.230842 -0.325648 -0.042102 ...  0.038772  0.054568 -0.39402 ]\n [ 0.006005  0.171988  0.404271 ... -0.438552  0.100765  0.623057]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2c92bc61ebfa43e8c64c17178f72e97c5a34f436",
        "_cell_guid": "520b0ab2-bcbf-4f45-85d3-1582bd8a9148",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 1-layer LSTM + 2-layer fully-connected MLP for sentiment classification\n\nencoder_inputs = Input(shape=(None, vector_dim))\nencoder = LSTM(vector_dim, return_state=True, name=\"encoder\")\nencoder_outputs, state_h, state_c = encoder(encoder_inputs) # Encoder_outputs are discarded\nencoder_states = [state_h, state_c] # state_h is the sentence vector we will be using\nl_dense = Dense(vector_dim*2, activation='relu')(state_h)\nl_dense_2 = Dense(vector_dim, activation='relu')(l_dense)\npreds = Dense(num_labels, activation='softmax')(l_dense)\n\nmodel_k = Model([encoder_inputs], preds)\nmodel_k.summary()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, None, 25)          0         \n_________________________________________________________________\nencoder (LSTM)               [(None, 25), (None, 25),  5100      \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                1300      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 102       \n=================================================================\nTotal params: 6,502\nTrainable params: 6,502\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "_uuid": "8b28032fb3900cb2efd71e85005eade4826a3bea",
        "_cell_guid": "b9f256c9-8fab-42a7-b83c-720a81852c8a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Compile and start training\nmodel_k.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\n# We feed the test data as the validation data in this Keras model to check out the overall accuracy\n# Validation data are NOT used in training per Keras design\n\n# We could use Keras to the save the best model using its checkpoint feature, which is not allowed on Kaggle\n# checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\n\nmodel_k.fit(encoder_input_data[:num_split], amazon_train_labels[:num_split],\n          batch_size = batch_size,\n          # epochs = epochs,\n          epochs=1,\n          validation_data=[encoder_input_data[num_split:], amazon_train_labels[num_split:]],\n          # callback = [checkpointer]\n           )\n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 40000 samples, validate on 10000 samples\nEpoch 1/1\n40000/40000 [==============================] - 202s 5ms/step - loss: 0.6707 - acc: 0.5783 - val_loss: 0.6330 - val_acc: 0.6474\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f5bc7d728d0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "af00a39cac652ce9b86d8d9a003c823c64a983c2"
      },
      "cell_type": "code",
      "source": "intermediate_layer_model = Model(inputs=model_k.input,\n                                 outputs=model_k.get_layer(\"encoder\").output[1])\n\nX = [encoder_input_data]\nintermediate_output = intermediate_layer_model.predict(X)\n# Save all the sentence vectors (not allowed on Kaggle)\n# np.savetxt(\"data/amazon/amazon_embedding_40k_conv1d_d\" +str(vector_dim)+ \".csv\", intermediate_output, delimiter=\",\")\nprint(intermediate_output.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c16ddc98f4deedf11268902c14a6fddcedb51f3a",
        "_cell_guid": "0a076cea-e7b8-43db-8f72-f0f966dd91ba",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"generating tsne graph\")\n# Generate the 2D tSNE graph using the 10k sentence vectors of the test data/labels\nfrom sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2).fit_transform(intermediate_output[num_split:])\nimport matplotlib.pyplot as plt\nplt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=sentiment_train[num_split:])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "920ef63a16c29064b62eb3c6017f956887f2af70",
        "_cell_guid": "2541e47c-1d81-428b-899a-7ed997e89dbe",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2bb480e472b2742f8c3620f21b0c1fb8735ff693",
        "_cell_guid": "113d869f-ffd9-4b2b-8c2e-210529f87535",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cdb72664cbc73f6af88dab6ac60f6fd964663d8b",
        "_cell_guid": "39d56b95-906f-4f6e-b63a-4077a7e7ec2a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}