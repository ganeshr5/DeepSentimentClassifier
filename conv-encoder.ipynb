{
  "cells": [
    {
      "metadata": {
        "_uuid": "3141bb8fcd563136148394d4148a86b2a6ab0082"
      },
      "cell_type": "markdown",
      "source": "To generate subword vectors, we use Google's sentencepiece (https://github.com/google/sentencepiece) with pretrained byte-pair encoding language models trained on wikipedia (https://github.com/bheinzerling/bpemb).\n\nThe cleaned sentences are lowercase only, unified all digits to 0, rid of URL, and with some additional manual anomaly removal.\n\nTo generate the subword sequences, we run this following line in Terminal (e.g. merge op = 3000)\nspm_encode --model en.wiki.bpe.op3000.model < amazon_sentences.txt.clean > amazon_sentences.bpe3000"
    },
    {
      "metadata": {
        "_cell_guid": "c5775e88-ef26-4191-89b5-d4fa934d11af",
        "_kg_hide-output": false,
        "_uuid": "eeb7789e8cf73a3300be745cad606eb079c3c0a7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Sentence Embedding with Conv1d\n# trained/tested with Amazon review dataset\n\n# Load the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Masking\nfrom keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\nfrom keras.callbacks import ModelCheckpoint\n\n# The dimensionality of all sentence vectors\nvector_dim = 25",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "7e33562d-746e-4691-a559-c1e7e4456661",
        "_uuid": "3106f9f9f578c8fc05b2546f2f09fbb5ed64f3cf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from gensim.models import KeyedVectors\nimport codecs\n\n# Load the pre-trained subwrod language model and subword sequences\n# Training data and test data are preemptively concatenated in one single training file for convenience\n# First 40k for training, last 10k for testing\n\nmodel = KeyedVectors.load_word2vec_format(\"../input/en.wiki.bpe.op3000.d\" + str(vector_dim) + \".w2v.bin\", binary=True)\n# f_train = codecs.open(\"../input/movie_phrases.bpe3000\", \"r\", \"utf-8\")\nf_train = codecs.open(\"../input/amazon_train_sentences.bpe3000\", \"r\", \"utf-8\")\namazon_train = [line for line in f_train.readlines() if line.strip()]\nf_train.close()\n\n# Same goes for loading the labels\nsentiment_train = np.genfromtxt('../input/amazon_train_labels.txt', delimiter=',')\n# sentiment_train = np.genfromtxt('../input/movie_labels.txt', delimiter=',')\nnum_labels = len(np.unique(sentiment_train))\n\nbatch_size = 256  # Batch size for training\nepochs = 10  # Number of epochs to train for\nnum_samples = 50000 # Total number of samples used\nnum_split = 40000\n\namazon_train = amazon_train[:num_samples] \nsentiment_train = sentiment_train[:num_samples]\namazon_train_labels = np_utils.to_categorical(sentiment_train, num_labels)\n\n# Due to memory limitation, we only take in the first 800 subwords of each sentence\n# for zero padding\nmax_encoder_seq_length = 800\n\ngc.collect()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "d7a47f2f-2563-4def-b6aa-09c7db8522e6",
        "_uuid": "1967f57489e12c8fa8c7c15773e045d9452ede14",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "encoder_input_data = np.zeros(\n    (num_samples, max_encoder_seq_length, vector_dim),\n    dtype='float32')\n\n# Feed in the subword vectors\nfor i in range(len(amazon_train)):\n    try:\n        input_vec = model[amazon_train[i].split()]\n    except KeyError:\n        # In case of non-existing vocabulary, we simply replace it with the previous subword\n        # Very rare\n        print(i)\n        input_vec = model[amazon_train[i-1].split()]\n    \n    for j in range(len(input_vec)):\n        encoder_input_data[i][j] = + input_vec[j][:]     ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3c66fa1b-fd2e-4662-abd9-a393df6facfd",
        "_uuid": "278a08b5be960bb6ced797080740c6dd41ca4930",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 3-layer 1D-CNN with maxpooling + 2-layer fully-connected MLP for sentiment classification\n\nsequence_input = Input(shape=(max_encoder_seq_length,vector_dim,), dtype='float32')\nl_cov1= Conv1D(vector_dim, 25, activation='relu')(sequence_input)\nl_pool1 = MaxPooling1D(3)(l_cov1)\nl_cov2 = Conv1D(vector_dim, 15, activation='relu')(l_pool1)\nl_pool2 = MaxPooling1D(3)(l_cov2)\nl_cov3 = Conv1D(vector_dim, 5, activation='relu')(l_pool2)\nl_pool3 = MaxPooling1D(40)(l_cov3)  # global max pooling\nl_flat = Flatten(name=\"sent_embedding\")(l_pool3)\nl_dense = Dense(vector_dim*2, activation='relu')(l_flat)\nl_dense_2 = Dense(vector_dim, activation='relu')(l_dense)\npreds = Dense(num_labels, activation='softmax')(l_dense)\n\nmodel_k = Model([sequence_input], preds)\nmodel_k.summary()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 800, 25)           0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 776, 25)           15650     \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 258, 25)           0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 244, 25)           9400      \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 81, 25)            0         \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, 77, 25)            3150      \n_________________________________________________________________\nmax_pooling1d_3 (MaxPooling1 (None, 1, 25)             0         \n_________________________________________________________________\nsent_embedding (Flatten)     (None, 25)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                1300      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 102       \n=================================================================\nTotal params: 29,602\nTrainable params: 29,602\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "5e707593-e7a5-4def-99de-792ba1230201",
        "_uuid": "0a4d7f5e74d03d0d2026bc3bff22f6caa2b54b60",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Compile and start training\nmodel_k.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\n# We feed the test data as the validation data in this Keras model to check out the overall accuracy\n# Validation data are NOT used in training per Keras design\n\n# We could use Keras to the save the best model using its checkpoint feature, which is not allowed on Kaggle\n# checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\n\nmodel_k.fit(encoder_input_data[:num_split], amazon_train_labels[:num_split],\n          batch_size=batch_size,\n          epochs=1,\n          # epochs = epochs,  \n          validation_data=[encoder_input_data[num_split:], amazon_train_labels[num_split:]],\n          # callback = [checkpointer]\n           )\n\n# Save the model (not allowed on Kaggle)\n# model_k.save('/amazon_40k_sample_d' + str(vector_dim) + '.h5')\n\n# Get the trained sentence vectors for sentimental analysis and tSNE\nintermediate_layer_model = Model(inputs=model_k.input,\n                                 outputs=model_k.get_layer(\"sent_embedding\").output)\n\nX = [encoder_input_data]\nintermediate_output = intermediate_layer_model.predict(X)\n# Save all the sentence vectors (not allowed on Kaggle)\n# np.savetxt(\"data/amazon/amazon_embedding_40k_conv1d_d\" +str(vector_dim)+ \".csv\", intermediate_output, delimiter=\",\")\nprint(intermediate_output.shape)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 40000 samples, validate on 10000 samples\nEpoch 1/1\n40000/40000 [==============================] - 16s 407us/step - loss: 0.6732 - acc: 0.5766 - val_loss: 0.6480 - val_acc: 0.6190\n(50000, 25)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "16cd196e-4054-4f47-a1d5-8cc171613abf",
        "_uuid": "22ac773e4a7103f885ec234fba3282e0c0317619",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Generate the 2D tSNE graph using the 10k sentence vectors of the test data/labels\nfrom sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2).fit_transform(intermediate_output[num_split:])\nimport matplotlib.pyplot as plt\nplt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=sentiment_train[num_split:])\nplt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}